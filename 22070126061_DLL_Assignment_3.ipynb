{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3acf7e3d",
   "metadata": {},
   "source": [
    "Name: Madhusudan Hasbe\\\n",
    "PRN: 22070126061\\\n",
    "TY AIML A3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a88babc",
   "metadata": {},
   "source": [
    "# ASSIGNMENT 3\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0edc91",
   "metadata": {},
   "source": [
    "\n",
    "# Feedforward Neural Network (from Scratch) for Iris Dataset\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "In this section, we implement a feedforward neural network from scratch using NumPy for the Iris dataset. The Iris dataset consists of 150 samples of iris flowers, with features like sepal length, sepal width, petal length, and petal width, and it has 3 classes (setosa, versicolor, virginica).\n",
    "\n",
    "We will use the following neural network structure:\n",
    "- Input layer: 4 features\n",
    "- Hidden layer: 5 units\n",
    "- Output layer: 3 units (one for each class)\n",
    "\n",
    "### Dataset Preprocessing\n",
    "We'll normalize the features and convert the labels into one-hot encoding.\n",
    "\n",
    "### Neural Network Implementation (FeedForwardNN)\n",
    "Below is the implementation of the `FeedForwardNN` class from scratch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0a16b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dfc9754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# One hot encode the labels\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_onehot = encoder.fit_transform(y)\n",
    "\n",
    "# Standardize the dataset\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_onehot, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19fba581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the FeedForwardNN class (from scratch)\n",
    "class FeedForwardNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.weights1 = np.random.randn(input_size, hidden_size)\n",
    "        self.bias1 = np.zeros((1, hidden_size))\n",
    "        self.weights2 = np.random.randn(hidden_size, output_size)\n",
    "        self.bias2 = np.zeros((1, output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.hidden_layer_activation = np.dot(x, self.weights1) + self.bias1\n",
    "        self.hidden_layer_output = self.sigmoid(self.hidden_layer_activation)\n",
    "        self.output_layer_activation = np.dot(self.hidden_layer_output, self.weights2) + self.bias2\n",
    "        predictions = self.sigmoid(self.output_layer_activation)\n",
    "        return predictions\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def backward(self, x, y, output, lr):\n",
    "        output_error = output - y\n",
    "        d_weights2 = np.dot(self.hidden_layer_output.T, output_error)\n",
    "        \n",
    "        hidden_layer_error = np.dot(output_error, self.weights2.T) * self.sigmoid_derivative(self.hidden_layer_activation)\n",
    "        d_weights1 = np.dot(x.T, hidden_layer_error)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.weights1 -= lr * d_weights1\n",
    "        self.bias1 -= lr * np.sum(hidden_layer_error, axis=0, keepdims=True)\n",
    "        self.weights2 -= lr * d_weights2\n",
    "        self.bias2 -= lr * np.sum(output_error, axis=0, keepdims=True)\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2457ba38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "nn = FeedForwardNN(input_size=4, hidden_size=5, output_size=3)\n",
    "\n",
    "# Training loop\n",
    "epochs = 500\n",
    "lr = 0.01\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    output = nn.forward(X_train)\n",
    "    nn.backward(X_train, y_train, output, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f271450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for Iris Dataset (from scratch): 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "predictions = nn.forward(X_test)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predicted_classes == true_classes)\n",
    "print(f\"Test Accuracy for Iris Dataset (from scratch): {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a635d9fc",
   "metadata": {},
   "source": [
    "We got 100% Accuracy on our Feed Forward network built from scratch on Iris Dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50644f4",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5b4b27",
   "metadata": {},
   "source": [
    "\n",
    "# Feedforward Neural Network (with TensorFlow and Keras) for Titanic Dataset\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "In this section, we will implement a feedforward neural network using TensorFlow and Keras for the Titanic dataset. The Titanic dataset contains information about the passengers, including their survival status.\n",
    "\n",
    "### Dataset Preprocessing\n",
    "We will preprocess the data by handling missing values, encoding categorical features, and normalizing the data.\n",
    "\n",
    "### Neural Network Implementation (with TensorFlow/Keras)\n",
    "We'll build a simple feedforward neural network using Keras with the following structure:\n",
    "- Input layer: corresponds to the number of features after preprocessing\n",
    "- Hidden layer: 16 units with ReLU activation\n",
    "- Output layer: 1 unit with sigmoid activation for binary classification (survived or not).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a7ce68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "316da2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Preprocess the data\n",
    "data = data[['Pclass', 'Sex', 'Age', 'Fare', 'Survived']]\n",
    "data['Age'].fillna(data['Age'].mean(), inplace=True)\n",
    "\n",
    "# Convert categorical features\n",
    "data['Sex'] = LabelEncoder().fit_transform(data['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be618319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = data.drop('Survived', axis=1)\n",
    "y = data['Survived']\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fe9e5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a feedforward neural network using Keras\n",
    "model = Sequential([\n",
    "    Dense(16, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8326fb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 2ms/step - loss: 0.7242 - accuracy: 0.5646\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6815 - accuracy: 0.6433\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6462 - accuracy: 0.6685\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6166 - accuracy: 0.6896\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.5904 - accuracy: 0.7205\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5682 - accuracy: 0.7416\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5482 - accuracy: 0.7542\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.5295 - accuracy: 0.7612\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5136 - accuracy: 0.7711\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5005 - accuracy: 0.7781\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4895 - accuracy: 0.7823\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4803 - accuracy: 0.7837\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4728 - accuracy: 0.7767\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4668 - accuracy: 0.7823\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4620 - accuracy: 0.7893\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4580 - accuracy: 0.7865\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4554 - accuracy: 0.7865\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4528 - accuracy: 0.7823\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4508 - accuracy: 0.7837\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4498 - accuracy: 0.7809\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4479 - accuracy: 0.7823\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4471 - accuracy: 0.7809\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4451 - accuracy: 0.7837\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4440 - accuracy: 0.7823\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4429 - accuracy: 0.7795\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4422 - accuracy: 0.7851\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4413 - accuracy: 0.7809\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4404 - accuracy: 0.7851\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4397 - accuracy: 0.7879\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4390 - accuracy: 0.7865\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4383 - accuracy: 0.7837\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4375 - accuracy: 0.7837\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4368 - accuracy: 0.7851\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4362 - accuracy: 0.7865\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4354 - accuracy: 0.7907\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4351 - accuracy: 0.7893\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4346 - accuracy: 0.7935\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4339 - accuracy: 0.7992\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4335 - accuracy: 0.8006\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4327 - accuracy: 0.8006\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4325 - accuracy: 0.8006\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4319 - accuracy: 0.8006\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4314 - accuracy: 0.7978\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4310 - accuracy: 0.8020\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4305 - accuracy: 0.8034\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4300 - accuracy: 0.8020\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4301 - accuracy: 0.8076\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4293 - accuracy: 0.8090\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4290 - accuracy: 0.8104\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4290 - accuracy: 0.8076\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1f571bee650>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f088d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4426 - accuracy: 0.7765\n",
      "Test Accuracy for Titanic Dataset (with TensorFlow/Keras): 77.65%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy for Titanic Dataset (with TensorFlow/Keras): {test_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b95694a",
   "metadata": {},
   "source": [
    "We got 77.65% Accuracy for Titanic Datset using TensorFlow/Keras."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
